{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Speculative decoding skeleton (exercise)\n",
        "\n",
        "Reference: [../speculative_decoding.ipynb](../speculative_decoding.ipynb). Paper: https://arxiv.org/abs/2211.17192\n",
        "\n",
        "**Goals:**\n",
        "- **generate_vanilla** (optional): Autoregressive loop â€” one forward per token; argmax logits for next token; append until max_tokens or EOS.\n",
        "- **generate_specdec_greedy**: (1) Draft phase: run draft model autoregressively to get `num_draft_tokens` candidates. (2) Target verification: one forward of target on (prompt + draft tokens). (3) Accept/reject: compare target next-token predictions to draft; find first disagreement; accept prefix + one target token; repeat. One target forward per block."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Setup: load draft and target models (same as reference). Use smaller models if needed for testing.\n",
        "DRAFT_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "TARGET_NAME = \"Qwen/Qwen2.5-3B-Instruct\"\n",
        "target = AutoModelForCausalLM.from_pretrained(TARGET_NAME, device_map=\"auto\", trust_remote_code=True)\n",
        "draft = AutoModelForCausalLM.from_pretrained(DRAFT_NAME, device_map=\"auto\", trust_remote_code=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(DRAFT_NAME, trust_remote_code=True)\n",
        "prompt = \"Hello, how are you?\"\n",
        "\n",
        "def dummy_generate(*args, **kwargs):\n",
        "    raise NotImplementedError(\"generate() disabled\")\n",
        "draft.generate = dummy_generate\n",
        "target.generate = dummy_generate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from time import time\n",
        "\n",
        "def cat(a, b, c=None):\n",
        "    if c is not None:\n",
        "        return torch.cat([a, b, c], dim=1)\n",
        "    return torch.cat([a, b], dim=1)\n",
        "\n",
        "def generate_vanilla(model, tokenizer, prompt, max_tokens=256, profile=False):\n",
        "    # TODO (optional): Autoregressive loop. input_ids = tokenizer(prompt).input_ids.cuda(). While count < max_tokens:\n",
        "    #   logits = model(input_ids).logits; next_token = argmax(logits[:, -1:, :]); input_ids = cat(input_ids, next_token); break if EOS.\n",
        "    raise NotImplementedError(\"TODO: implement generate_vanilla\")\n",
        "\n",
        "def generate_specdec_greedy(draft, target, tokenizer, prompt, num_draft_tokens=7, max_tokens=256, profile=False):\n",
        "    # TODO: (1) Draft: from input_ids, run draft autoregressively num_draft_tokens times -> draft_ids.\n",
        "    # (2) Target: full_seq = cat(input_ids, draft_ids); target_logits = target(full_seq).logits.\n",
        "    # (3) Accept/reject: target_pred_ids = argmax(target_logits from pos len(input_ids)-1). First index where draft_ids != target_pred_ids[:-1].\n",
        "    #     New input_ids = cat(input_ids, draft_ids[:, :first], target_pred_ids[:, first:first+1]). Repeat until max_tokens or EOS.\n",
        "    raise NotImplementedError(\"TODO: implement generate_specdec_greedy\")\n",
        "\n",
        "# After implementing: print(generate_vanilla(target, tokenizer, prompt, max_tokens=256))\n",
        "# print(generate_specdec_greedy(draft, target, tokenizer, prompt, max_tokens=256))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
