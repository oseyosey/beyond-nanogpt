{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KV cache skeleton (exercise)\n",
    "\n",
    "Reference: [../KV_cache.ipynb](../KV_cache.ipynb).\n",
    "\n",
    "**Goals:**\n",
    "- Implement **KVCache.update(layer, new_keys, new_values)**: write the new K/V into the preallocated cache at `current_length`; shapes are [B, nh, token_count, hd]. Cache is per-layer; keys/values have shape [B, nh, max_len, hd]. Advance `current_length` after append (caller does this in _prefill/generate).\n",
    "- In **Attention forward**: when `kv_cache` is not None and layer_idx is set, call `kv_cache.update(layer_idx, K, V)` and read K, V from the cache (slice by `kv_cache.current_length`) instead of using the full K, V from this forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: self-contained transformer (same as reference). Focus on the TODO cells below.\n",
    "import torch\n",
    "from typing import List\n",
    "\n",
    "ACT2FN = {\n",
    "    'relu': torch.nn.functional.relu,\n",
    "    'gelu': torch.nn.functional.gelu,\n",
    "    'silu': torch.nn.functional.silu,\n",
    "    'swish': torch.nn.functional.silu,\n",
    "}\n",
    "\n",
    "class Attention(torch.nn.Module):\n",
    "    def __init__(self, D=768, layer_idx=None, head_dim=64, causal=True, device=\"cuda\", gqa=False):\n",
    "        super().__init__()\n",
    "        self.D = D\n",
    "        self.head_dim = head_dim\n",
    "        self.gqa = gqa\n",
    "        assert D % head_dim == 0\n",
    "        self.nheads = D // head_dim\n",
    "        self.Wq = torch.nn.Linear(D, D)\n",
    "        self.Wk = torch.nn.Linear(D, D)\n",
    "        self.Wv = torch.nn.Linear(D, D)\n",
    "        self.causal = causal\n",
    "        self.Wo = torch.nn.Linear(D, D)\n",
    "        self.device = device\n",
    "        self.layer_idx = layer_idx\n",
    "\n",
    "    def forward(self, x: torch.Tensor, kv_cache=None):\n",
    "        B, S, D = x.shape\n",
    "        Q, K, V = self.Wq(x), self.Wk(x), self.Wv(x)\n",
    "        Q = Q.view(B, S, self.nheads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(B, S, self.nheads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(B, S, self.nheads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # TODO: If kv_cache is not None and self.layer_idx is not None:\n",
    "        #   Call kv_cache.update(self.layer_idx, K, V).\n",
    "        #   Then set K = kv_cache.keys[layer_idx][:, :, :kv_cache.current_length, :]\n",
    "        #   and V = kv_cache.values[layer_idx][:, :, :kv_cache.current_length, :]\n",
    "        if kv_cache is not None and self.layer_idx is not None:\n",
    "            raise NotImplementedError(\"TODO: update cache and read K, V from cache\")\n",
    "\n",
    "        scale = torch.sqrt(torch.tensor(self.head_dim, dtype=Q.dtype, device=self.device))\n",
    "        logits = (Q @ K.transpose(-2, -1)) / scale\n",
    "        if self.causal:\n",
    "            mask = torch.triu(torch.ones_like(logits), diagonal=1).bool()\n",
    "            logits = logits.masked_fill(mask, float('-inf'))\n",
    "        A = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        preout = torch.einsum('bnxy,bnyd->bnxd', A, V)\n",
    "        preout = preout.transpose(1, 2).reshape(B, S, -1)\n",
    "        return self.Wo(preout)\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, D, hidden_multiplier=4, act='swish', device=None):\n",
    "        super().__init__()\n",
    "        self.D = D\n",
    "        self.up_proj = torch.nn.Linear(D, D * hidden_multiplier)\n",
    "        self.down_proj = torch.nn.Linear(D * hidden_multiplier, D)\n",
    "        self.act = ACT2FN[act]\n",
    "    def forward(self, x):\n",
    "        return self.down_proj(self.act(self.up_proj(x)))\n",
    "\n",
    "class LN(torch.nn.Module):\n",
    "    def __init__(self, D, eps=1e-9, device=None):\n",
    "        super().__init__()\n",
    "        self.mean_scale = torch.nn.Parameter(torch.zeros(D))\n",
    "        self.std_scale = torch.nn.Parameter(torch.ones(D))\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        std = (x.var(dim=-1, keepdim=True) + 1e-9)**0.5\n",
    "        return (x - mean) / std * self.std_scale + self.mean_scale\n",
    "\n",
    "class TransformerLayer(torch.nn.Module):\n",
    "    def __init__(self, D, gqa=False, device=None):\n",
    "        super().__init__()\n",
    "        self.attn = Attention(D, gqa=gqa, device=device or torch.device(\"cuda\"))\n",
    "        self.mlp = MLP(D, device=device)\n",
    "        self.ln1 = LN(D, device=device)\n",
    "        self.ln2 = LN(D, device=device)\n",
    "    def forward(self, x, kv_cache=None):\n",
    "        x = x + self.attn(self.ln1(x), kv_cache=kv_cache)\n",
    "        return x + self.mlp(self.ln2(x))\n",
    "\n",
    "class PositionalEmbedding(torch.nn.Module):\n",
    "    def __init__(self, max_seq_len, D, device=None):\n",
    "        super().__init__()\n",
    "        self.pos_embedding = torch.nn.Parameter(torch.randn(max_seq_len, D))\n",
    "    def forward(self, x):\n",
    "        B, S, D = x.shape\n",
    "        return x + self.pos_embedding[:S]\n",
    "\n",
    "class EmbeddingLayer(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, D, device=None):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Parameter(torch.randn(vocab_size, D))\n",
    "    def forward(self, x):\n",
    "        return self.embedding[x]\n",
    "\n",
    "class UnembeddingLayer(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, D, device=None):\n",
    "        super().__init__()\n",
    "        self.unembedding = torch.nn.Linear(D, vocab_size)\n",
    "    def forward(self, x):\n",
    "        return self.unembedding(x)\n",
    "\n",
    "class Transformer(torch.nn.Module):\n",
    "    def __init__(self, depth, hidden_dim, vocab_size, max_seq_len=16384, device=None, gqa=False):\n",
    "        super().__init__()\n",
    "        self.depth = depth\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.emb = EmbeddingLayer(vocab_size, hidden_dim, device=device)\n",
    "        self.pos_emb = PositionalEmbedding(max_seq_len, hidden_dim, device=device)\n",
    "        self.unemb = UnembeddingLayer(vocab_size, hidden_dim, device=device)\n",
    "        self.layers = torch.nn.ModuleList([TransformerLayer(hidden_dim, gqa, device=device) for _ in range(depth)])\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            layer.attn.layer_idx = i\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x, kv_cache=None):\n",
    "        x = self.emb(x)\n",
    "        if kv_cache is not None:\n",
    "            pos_offset = kv_cache.current_length\n",
    "            pos_emb = self.pos_emb.pos_embedding[pos_offset: pos_offset + x.size(1)].unsqueeze(0)\n",
    "            x = x + pos_emb\n",
    "        else:\n",
    "            x = self.pos_emb(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, kv_cache=kv_cache)\n",
    "        return self.unemb(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "KV CACHE: preallocated tensor we fill during autoregressive decoding.\n",
    "Prefill = process full prompt and populate cache. Decode = one token at a time, update cache with new K,V.\n",
    "Cache shape per layer: keys/values [B, nh, max_seq_len, head_dim]. Advance current_length after append.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KVCache:\n",
    "    \"\"\"\n",
    "    Preallocated K/V cache. keys and values: list of tensors [B, num_heads, max_seq_len, head_dim] per layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layers: int, batch_size: int, num_heads: int, head_dim: int, max_seq_len: int, device='cuda'):\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.device = device\n",
    "        self.current_length = 0\n",
    "        # Preallocate: one tensor per layer for keys and values\n",
    "        self.keys = [torch.empty(batch_size, num_heads, max_seq_len, head_dim, device=device) for _ in range(num_layers)]\n",
    "        self.values = [torch.empty(batch_size, num_heads, max_seq_len, head_dim, device=device) for _ in range(num_layers)]\n",
    "\n",
    "    def update(self, layer: int, new_keys: torch.Tensor, new_values: torch.Tensor):\n",
    "        # TODO: Write new_keys and new_values into self.keys[layer] and self.values[layer]\n",
    "        # at positions [:, :, seq_offset : seq_offset + token_count, :]. seq_offset = self.current_length.\n",
    "        raise NotImplementedError(\"TODO: implement KVCache.update\")\n",
    "\n",
    "class TransformerGenerator:\n",
    "    def __init__(self, model: Transformer, max_seq_len: int = 4096):\n",
    "        self.model = model\n",
    "        self.device = next(model.parameters()).device\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.kv_cache = None\n",
    "\n",
    "    def _initialize_cache(self, batch_size: int):\n",
    "        attn = self.model.layers[0].attn\n",
    "        num_heads = getattr(attn, 'num_kv_heads', None) or attn.nheads\n",
    "        self.kv_cache = KVCache(\n",
    "            self.model.depth, batch_size, num_heads, attn.head_dim,\n",
    "            self.max_seq_len, self.device\n",
    "        )\n",
    "        self.model.kv_cache = self.kv_cache\n",
    "\n",
    "    def _prefill(self, prompt: List[int]):\n",
    "        prompt_tensor = torch.tensor(prompt, device=self.device).unsqueeze(0)\n",
    "        self._initialize_cache(prompt_tensor.size(0))\n",
    "        _ = self.model(prompt_tensor, kv_cache=self.kv_cache)\n",
    "        self.kv_cache.current_length = prompt_tensor.size(1)\n",
    "        return self.kv_cache\n",
    "\n",
    "    def generate(self, prompt: List[int], max_new_tokens: int):\n",
    "        kv_cache = self._prefill(prompt)\n",
    "        generated = list(prompt)\n",
    "        for _ in range(max_new_tokens):\n",
    "            input_tensor = torch.tensor([[generated[-1]]], device=self.device)\n",
    "            logits = self.model(input_tensor, kv_cache=kv_cache)\n",
    "            next_token = int(torch.argmax(logits[:, -1, :], dim=-1).item())\n",
    "            generated.append(next_token)\n",
    "            kv_cache.current_length += 1\n",
    "        return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After implementing KVCache.update and the Attention kv_cache block, run a quick sanity check:\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = Transformer(depth=2, hidden_dim=64, vocab_size=256, device=device).to(device)\n",
    "model.eval()\n",
    "gen = TransformerGenerator(model, max_seq_len=128)\n",
    "with torch.no_grad():\n",
    "    out = gen.generate([1, 2, 3], max_new_tokens=5)\n",
    "print(\"Generated token ids:\", out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
